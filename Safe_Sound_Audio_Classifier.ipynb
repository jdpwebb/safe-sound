{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Safe Sound Audio Classifier",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "WnONlHuUj0aO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaLmnCaeCVor",
        "colab_type": "text"
      },
      "source": [
        "#Train an Audio Classifier for the Azure Sphere\n",
        "\n",
        "Notebook authored by Jeremy Webb\n",
        "\n",
        "This notebook will walkthrough how to setup and train an audio classifier model that will run on the Microsoft Azure Sphere using the Embedded Learning Library.\n",
        "\n",
        "**To train an audio classifier, read and run each cell below by clicking on the play button on the left side.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnONlHuUj0aO",
        "colab_type": "text"
      },
      "source": [
        "## License"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYH0YtPgP0-W",
        "colab_type": "text"
      },
      "source": [
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Jeremy Webb\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcM2_ce4A5mC",
        "colab_type": "text"
      },
      "source": [
        "# Setup and Install Prerequisites\n",
        "\n",
        "To train a machine learning model to run on the Azure Sphere, the Embedded Learning Library (ELL) will be used. ELL is specifically developed by Microsoft for running machine learning models on resource constrained systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opYbXV1yCgXy",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Start by cloning the library. Note that v3.0.2 is specified because that version was tested with the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35pAh5EGtUUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ell_dir = \"/content/ELL\"\n",
        "ell_scripts_dir = ell_dir + \"/tools/utilities/pythonlibs/audio/training/\"\n",
        "!git clone --branch v3.0.2 https://github.com/Microsoft/ELL.git {ell_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF84wWIZAted",
        "colab_type": "text"
      },
      "source": [
        "Install ELL prerequisites."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9Id9W1Dtyk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sh -c 'echo deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-8 main >> /etc/apt/sources.list'\n",
        "!sh -c 'echo deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bionic-8 main >> /etc/apt/sources.list'\n",
        "!wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add -\n",
        "!apt-get -y update\n",
        "!apt-get install -y gcc-8 g++-8 cmake libedit-dev zlibc zlib1g zlib1g-dev make\n",
        "!apt-get install -y libopenblas-dev doxygen\n",
        "!apt-get install python-pyaudio python3-pyaudio\n",
        "!apt-get install llvm-8 -y\n",
        "!pip install onnx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yOWDpM5wBGL",
        "colab_type": "text"
      },
      "source": [
        "Install SWIG, which is used to generate Python to C++ interfaces for ELL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5AxFz5cv6JO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!curl -O --location http://prdownloads.sourceforge.net/swig/swig-4.0.0.tar.gz\n",
        "!tar zxvf swig-4.0.0.tar.gz\n",
        "%cd swig-4.0.0\n",
        "!./configure --without-pcre && make && make install\n",
        "%cd /content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1e5sW3XCSyh",
        "colab_type": "text"
      },
      "source": [
        "Build ELL and generate the Python libraries. This takes a while to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pSsR3vux5Lw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ell_build_dir = ell_dir + \"/build\"\n",
        "!mkdir {ell_build_dir}\n",
        "%cd {ell_build_dir}\n",
        "!cmake ..\n",
        "!make\n",
        "!make _ELL_python\n",
        "%cd /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4VPtnexUJCO",
        "colab_type": "text"
      },
      "source": [
        "# Setup the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9FLjdHQDy1X",
        "colab_type": "text"
      },
      "source": [
        "Setup paths and directories for the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS1LCjTjUNT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "base_dir = Path(\"/content/security/\")\n",
        "train_data_dir = base_dir / \"audio/\"\n",
        "train_dir = base_dir / \"models/\"\n",
        "drive_dir = Path(\"/content/drive/My Drive/Colab Notebooks/\")\n",
        "drive_data_dir = drive_dir / \"Data\" / \"azuresphere\"\n",
        "!mkdir -p \"{train_data_dir}\"\n",
        "!mkdir -p \"{train_dir}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwUyDucFGRd7",
        "colab_type": "text"
      },
      "source": [
        "Mount your Google Drive to load or store training data. This step is not necessary if you are not planning on storing training data in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFNzapRQovV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib import Path\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "audio_data_file = 'security_sounds.zip'\n",
        "audio_data_path = drive_data_dir / audio_data_file\n",
        "train_data_found = False\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "if not drive_dir.exists():\n",
        "  print(\"Note: Create 'Colab Notebooks' directory in drive root to proceed.\")\n",
        "elif not drive_data_dir.exists():\n",
        "  print(\"Note: Create 'azuresphere' directory inside 'Colab Notebooks'\" \\\n",
        "    \" to proceed.\")\n",
        "if not audio_data_path.exists():\n",
        "  print(\"Warning: Upload compressed data missing.\"\n",
        "    \" Download the data in the next step.\")\n",
        "else:\n",
        "  with zipfile.ZipFile(audio_data_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(train_data_dir)\n",
        "    train_data_found = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3cbxRyGJhQ8",
        "colab_type": "text"
      },
      "source": [
        "## Download Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu1wnRb7WybP",
        "colab_type": "text"
      },
      "source": [
        "Training data will be built from a variety of samples collected from Freesound.org. Freesound hosts a large collection of recorded sounds in many different kinds of categories.\n",
        "\n",
        "You will need to [register for an account](https://freesound.org/home/register/) and then [request an API access key](https://freesound.org/apiv2/apply). Copy and paste the access key into the \"api_key\" variable below.\n",
        "\n",
        "If you wish to use different data than the defaults, edit the \"labels_to_download\" dictionary, and add a key as the training category paired with a FreesoundQuery as the value. A FreesoundQuery takes in a string of words to use for the query, and a space separated string of tags to use to filter the query. The tags can be empty. A good way to figure out what query to use is to go to the [Freesound website](https://freesound.org) and use the search box to try various queries. You can examine the results to see if the returned samples fit your intended category. For best results, you should ensure that there are at least 400 samples for each category, but more will result in a more accurate model.\n",
        "\n",
        "Note that machine learning models are only as good as the data they are trained from. Ideally, you would ensure that each sample has only the noises you are looking to train and no other noises or extra silence. However, because almost anyone can upload audio samples to Freesound, there is no guarantee that mislabeled or poor samples are not mixed in with the data collected by the script below. To get top-quality results, the data should be cleaned and checked for accuracy. This obviously takes a lot of effort, but the good news is that decent results can be obtained without doing this.\n",
        "\n",
        "The script below will process each FreesoundQuery and download the resulting samples, saving them in a directory specified by the dictionary key name. It is not necessary to run the cell below if the training data has been loaded from Google Drive in the previous step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cg5YqAcRq75C",
        "colab": {}
      },
      "source": [
        "# download dataset from FreeSound since it wasn't found in previous step\n",
        "if not train_data_found:\n",
        "  import math\n",
        "  from pathlib import Path\n",
        "  import subprocess\n",
        "  import json\n",
        "  import requests\n",
        "\n",
        "  api_key = \"XXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
        "  max_download = 600  # max number of samples per category to download\n",
        "\n",
        "  class FreesoundQuery:\n",
        "    def __init__(self, query='', tags='', segment=False):\n",
        "      self.query = query\n",
        "      self.tags = tags\n",
        "      self.segment = segment\n",
        "\n",
        "  labels_to_download = {\n",
        "    'window_break': [FreesoundQuery('shatter', 'glass'),\n",
        "                      FreesoundQuery('window breaking', 'glass window')],\n",
        "    #'door_break': [FreesoundQuery('wood break', 'wood Wood')],\n",
        "    'gunshot': [FreesoundQuery('gunshot')],\n",
        "    'background_noise': [FreesoundQuery('silence', segment=True), \n",
        "                        FreesoundQuery('background-noise indoor', segment=True)\n",
        "                          ]\n",
        "  }\n",
        "\n",
        "\n",
        "  audio_codec = 'pcm_s16le'\n",
        "  audio_container = '.wav'\n",
        "\n",
        "  page_size = 150\n",
        "  search_url = 'http://freesound.org/apiv2/search/text/?' \\\n",
        "              '&query={query}&token={api_token}&tag={tag}' \\\n",
        "              '&page={page}&page_size={page_size}' \\\n",
        "              '&filter=license:(\"Attribution\" OR \"Creative Commons 0\")' \\\n",
        "              '&filter=duration:[* TO 30]' \\\n",
        "              '&fields=id,name,previews,username'\n",
        "  info_url = 'http://freesound.org/apiv2/sounds/{sound_id}/?&token={api_token}'\n",
        "  download_url = '{url}?&token={api_token}'\n",
        "\n",
        "  # get list of all audio clips that match the query\n",
        "  def get_audio_list(query, tags, max_files):\n",
        "    max_pages = max_files / page_size\n",
        "    page_count = 1\n",
        "    url_list = []\n",
        "    tag = ' '.join(tags)\n",
        "    url = search_url.format(query=query, api_token=api_key,\n",
        "                            tag=tag, page=1, page_size=page_size)\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    if(response.status_code >= 300 or len(response.json()) == 0):\n",
        "      # when the page is invalid, a 300 redirect is returned\n",
        "      # we don't want to do anymore processing after this\n",
        "      print(f\"Error encountered processing query: {query}\")\n",
        "      return None\n",
        "    data = response.json()\n",
        "    url_list.extend(data['results'])\n",
        "    while data['next'] and page_count < max_pages:\n",
        "      page_count += 1\n",
        "      url = data['next'] + f'&token={api_key}'\n",
        "      response = requests.get(url)\n",
        "      response.raise_for_status()\n",
        "      if(response.status_code >= 300 or len(response.json()) == 0):\n",
        "        # when the page is invalid, a 300 redirect is returned\n",
        "        # we don't want to do anymore processing after this\n",
        "        print(f\"Error encountered processing query: {query}\")\n",
        "        return None\n",
        "      data = response.json()\n",
        "      url_list.extend(data['results'])\n",
        "    return url_list\n",
        "\n",
        "  def download_audio(audio_data, label, store_dir, segment=False):\n",
        "    store_dir.mkdir(parents=True, exist_ok=True)\n",
        "    label = label.replace(' ', '_')\n",
        "    sample_name = \"\"\n",
        "    if segment:\n",
        "      sample_name = \"%d\"\n",
        "    audio_filename = str(audio_data['id']) + \"_\" + label \\\n",
        "    + sample_name + audio_container\n",
        "    audio_filepath = store_dir / audio_filename\n",
        "\n",
        "    audio_url = download_url.format(\n",
        "        url=audio_data['previews']['preview-hq-ogg'],\n",
        "        api_token=api_key\n",
        "        )\n",
        "    if segment:\n",
        "      if Path(str(audio_filepath) % 0).exists():\n",
        "        # file already downloaded\n",
        "        return True\n",
        "      audio_dl_cmd = ['ffmpeg', '-n',\n",
        "          #'-ss', str(sound_start),  # The beginning of the trim window\n",
        "          '-i', audio_url,          # audio URL\n",
        "          #'-t', str(duration),      # total duration of the segment\n",
        "          #'-vn',                    # suppress the video stream\n",
        "          '-ac', '1',               # set the number of channels\n",
        "          '-sample_fmt', 's16',     # bit depth\n",
        "          '-acodec', audio_codec,   # output encoding\n",
        "          '-ar', '16000',           # audio sample rate\n",
        "          '-threads', '1',\n",
        "          '-f', 'segment',\n",
        "          '-segment_time', '5',     # split the audio into 5 second chunks\n",
        "          str(audio_filepath)\n",
        "          ]\n",
        "    else:\n",
        "      if audio_filepath.exists():\n",
        "        # no need to download the file again\n",
        "        return True\n",
        "      audio_dl_cmd = ['ffmpeg', '-n',\n",
        "          '-i', audio_url,          # audio URL\n",
        "          '-ac', '1',               # set the number of channels\n",
        "          '-sample_fmt', 's16',     # bit depth\n",
        "          '-acodec', audio_codec,   # output encoding\n",
        "          '-ar', '16000',           # audio sample rate\n",
        "          '-threads', '1',\n",
        "          str(audio_filepath)\n",
        "          ]\n",
        "\n",
        "    attribution = f\"Downloading sample \\\"{audio_data['name']}\\\" \" \\\n",
        "    f\"provided by {audio_data['username']} from freesound.org\"\n",
        "    print(attribution)\n",
        "    result = subprocess.run(audio_dl_cmd)\n",
        "    return result.returncode == 0\n",
        "\n",
        "  def download_dataset(labels_dict):\n",
        "    for label, queries in labels_dict.items():\n",
        "      max_files = max_download / len(queries)\n",
        "      for query in queries:\n",
        "        data_list = get_audio_list(query.query, query.tags, max_files)\n",
        "        for data in data_list:\n",
        "          result = download_audio(\n",
        "              data, label, Path(train_data_dir / label), query.segment\n",
        "              )\n",
        "          if not result:\n",
        "            print(f\"Error downloading: {data['name']}\")\n",
        "\n",
        "  download_dataset(labels_to_download)\n",
        "  print(\"All done!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t91qNSoKJ7JU",
        "colab_type": "text"
      },
      "source": [
        "## Store Training Data\n",
        "\n",
        "It's recommended to compress and store the training data in Google Drive if you plan on using this document again in the future. This will allow you to skip downloading the data again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7P8vO2aLM1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "import tempfile\n",
        "from google.colab import files\n",
        "\n",
        "# delete zip file in drive if it already exists\n",
        "if audio_data_path.exists():\n",
        "  audio_data_path.unlink()\n",
        "# compress train data into a zip file and store in drive\n",
        "with tempfile.TemporaryDirectory() as tempdir:\n",
        "  zipf = tempdir + \"/\" + audio_data_path.stem\n",
        "  shutil.make_archive(zipf, 'zip', train_data_dir)\n",
        "  shutil.move(zipf + \".zip\", drive_data_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI2vM9zwKF5v",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Data for Training\n",
        "\n",
        "Select the training, validation, and testing data by randomly choosing 64 samples for testing and validation, and then using the rest for training. Note that simple random selection is used so it is possible that some samples on each list are duplicated, but this is unlikely to have a large effect on training or testing, as long as the amount of data is large enough.\n",
        "\n",
        "The cell below generates the validation and testing data lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy3TF8zz0ith",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "\n",
        "test_list = []\n",
        "val_list = []\n",
        "num_to_select = 64\n",
        "for (root,dirs,files) in os.walk(train_data_dir):\n",
        "  category_dir = Path(root).relative_to(train_data_dir)\n",
        "  if len(files) > 0 and str(category_dir) != \".\":\n",
        "    for i in range(num_to_select):\n",
        "      # this method may introduce duplicates, but its unlikely\n",
        "      # and impact won't be that bad\n",
        "      choice = random.choice(files)\n",
        "      test_list.append(str(category_dir) + \"/\" + choice)\n",
        "      choice = random.choice(files)\n",
        "      val_list.append(str(category_dir) + \"/\" + choice)\n",
        "with open(train_data_dir / 'testing_list.txt', 'w') as filehandle:\n",
        "    for listitem in test_list:\n",
        "        filehandle.write('%s\\n' % listitem)\n",
        "with open(train_data_dir / 'validation_list.txt', 'w') as filehandle:\n",
        "    for listitem in val_list:\n",
        "        filehandle.write('%s\\n' % listitem)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWENy6kMh502",
        "colab_type": "text"
      },
      "source": [
        "Generate the training data list and limit each category to num_files_to_use. This variable is used to ensure the training dataset is balanced with the same amount of files per category. If you have more samples in each category, increase this setting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivrC58ITdxsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "\n",
        "num_files_to_use = 350 #@param {type: \"number\"}\n",
        "\n",
        "%cd {train_dir}\n",
        "!python {ell_scripts_dir}make_training_list.py \\\n",
        "          --wav_files {train_data_dir} \\\n",
        "          --max_files_per_directory {num_files_to_use}\n",
        "%cd /content/\n",
        "shutil.copy(train_data_dir / \"categories.txt\", train_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YkoueIrLtxy",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Audio Classifier Model\n",
        "\n",
        "In this section, the audio classifier model will be setup for training. First you'll create and compile a featurizer, and then apply that featurizer to the training and testing data. Once the data has been preprocessed, the model is ready to be trained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSz5y8VfMCN1",
        "colab_type": "text"
      },
      "source": [
        "## Create Featurizer\n",
        "\n",
        "Create and compile the featurizer by running the cells below.\n",
        "\n",
        "The featurizer takes in a number of audio datapoints, in this case 512, and essentially creates a fingerprint for that set of datapoints. This fingerprint can be learned and classified by a machine learning model. It can also make the model run faster because it reduces the dimensionality of the input data for the model. For this classifier, the Mel Frequency Cepstrum Coefficients (MFCC) will be used. You can read about [MFCCs on Wikipedia](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSLb-5YAkBjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd {train_dir}\n",
        "!python {ell_scripts_dir}make_featurizer.py --sample_rate 16000 \\\n",
        "          --window_size 512 \\\n",
        "          --input_buffer_size 512 \\\n",
        "          --filterbank_type mel \\\n",
        "          --filterbank_size 80 \\\n",
        "          --filterbank_nfft 512 --nfft 512 --log \\\n",
        "          --auto_scale\n",
        "!python {ell_dir}/tools/wrap/wrap.py --model_file featurizer.ell \\\n",
        "  --outdir compiled_featurizer --module_name mfcc\n",
        "!{ell_dir}/build/bin/print -imap featurizer.ell\n",
        "%cd /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O55k23wE5Bgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd {train_dir}\n",
        "!mkdir compiled_featurizer/build\n",
        "!cd compiled_featurizer/build && cmake .. && make\n",
        "%cd /content/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vLRnkkQMKn_",
        "colab_type": "text"
      },
      "source": [
        "## Create Features from Data\n",
        "\n",
        "Using the featurizer created above, transform all the training, validation, and testing data into a set of features that can be fed into the machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9gtP0pi8dtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd {train_dir}\n",
        "!python {ell_scripts_dir}make_dataset.py \\\n",
        "          --list_file {train_data_dir / \"training_list.txt\"} \\\n",
        "          --featurizer compiled_featurizer/mfcc \\\n",
        "          --window_size 40 --shift 40\n",
        "!python {ell_scripts_dir}make_dataset.py \\\n",
        "          --list_file {train_data_dir / \"validation_list.txt\"} \\\n",
        "          --featurizer compiled_featurizer/mfcc \\\n",
        "          --window_size 40 --shift 40\n",
        "!python {ell_scripts_dir}make_dataset.py \\\n",
        "          --list_file {train_data_dir / \"testing_list.txt\"} \\\n",
        "          --featurizer compiled_featurizer/mfcc \\\n",
        "          --window_size 40 --shift 40"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIAvIouZqOCQ",
        "colab_type": "text"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "Create and train the machine learning model on the featurized data. The number of neurons in the hidden layers can be adjusted by changing the hidden_units number. If you have enough data, more units will allow the model to more accurately learn to classify the categories, but will result in a larger model. If the model is too large, it will not be able to run on the Azure Sphere. A larger model is also easier to overfit.\n",
        "\n",
        "The number of training iterations can be adjusted by setting the epochs value. A higher number of epochs will teach the classifier to more accurately model the training data, but too high will cause the model to overfit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB37gqIp_Unq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python {ell_scripts_dir}train_classifier.py \\\n",
        "          --architecture GRU --use_gpu \\\n",
        "          --dataset {train_data_dir} \\\n",
        "          --categories {train_dir / \"categories.txt\"} \\\n",
        "          --outdir {train_dir} \\\n",
        "          --filename classifier \\\n",
        "          --hidden_units 110 \\\n",
        "          --epochs 30 \\\n",
        "          --normalize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PO--dKMMt_N",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Model for Running on Azure Sphere\n",
        "\n",
        "To adjust the audio classifier so it runs on the Azure Sphere, it will first be converted to the ELL format. The ELL model can then be tested against the testing data to compare the accuracy of the ELL model against the original. Finally, it will be compiled for running on the Azure Sphere."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaRcOROXqGAs",
        "colab_type": "text"
      },
      "source": [
        "## Convert ONNX Model to ELL Format\n",
        "\n",
        "Convert the ONNX model generated in the previous step to an ELL model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ctrITgQqJ1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python {ell_dir}/tools/importers/onnx/onnx_import.py classifier.onnx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZJEax3NVXDC",
        "colab_type": "text"
      },
      "source": [
        "## Test ELL Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83p8aVErNoVZ",
        "colab_type": "text"
      },
      "source": [
        "Compile ELL Model for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iPkFKrg09ll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python {ell_dir}/tools/wrap/wrap.py --model_file classifier.ell --outdir SafeSound --module_name model\n",
        "!mkdir SafeSound/build\n",
        "!cd SafeSound/build && cmake .. && make"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMm3V8iwrHut",
        "colab_type": "text"
      },
      "source": [
        "Run the tests for ELL model below. You can compare the test accuracy to the training and testing accuracy shown in [Train the Model](#scrollTo=fIAvIouZqOCQ)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntJNwr9KYvs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python {ell_scripts_dir}test_ell_model.py \\\n",
        " --classifier {train_dir / \"SafeSound\" / \"model\"} \\\n",
        " --featurizer {train_dir / \"compiled_featurizer\" / \"mfcc\"} \\\n",
        " --sample_rate 16000 --list_file {train_data_dir / \"testing_list.txt\"} \\\n",
        " --categories {train_data_dir / \"categories.txt\"} --reset --auto_scale"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apRLeE9PVdy9",
        "colab_type": "text"
      },
      "source": [
        "## Compile ELL File for Use on Azure Sphere"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbXRoorxMMLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!{ell_dir}/build/bin/compile -imap featurizer.ell -cfn Filter -cmn mfcc \\\n",
        " --bitcode -od . --fuseLinearOps true --header --blas false --optimize true \\\n",
        " --target custom --numBits 32 --cpu cortex-a7 --triple armv7--linux-gnueabihf --features +vfp4,+d16\n",
        "!/usr/lib/llvm-8/bin/opt featurizer.bc -o featurizer.opt.bc -O3\n",
        "!/usr/lib/llvm-8/bin/llc featurizer.opt.bc -o featurizer.o -O3 -filetype=obj \\\n",
        " -mtriple=armv7--linux-gnueabihf -mcpu=cortex-a7 -relocation-model=pic -float-abi=hard -mattr=+vfp4,+d16\n",
        "!{ell_dir}/build/bin/compile -imap classifier.ell -cfn Predict -cmn model \\\n",
        " --bitcode -od . --fuseLinearOps true --header --blas false --optimize true \\\n",
        " --target custom --numBits 32 --cpu cortex-a7 --triple armv7--linux-gnueabihf --features +vfp4,+d16\n",
        "!/usr/lib/llvm-8/bin/opt classifier.bc -o classifier.opt.bc -O3\n",
        "!/usr/lib/llvm-8/bin/llc classifier.opt.bc -o classifier.o -O3 -filetype=obj \\\n",
        " -mtriple=armv7--linux-gnueabihf -mcpu=cortex-a7 -relocation-model=pic -float-abi=hard -mattr=+vfp4,+d16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZfTP87BYXL-",
        "colab_type": "text"
      },
      "source": [
        "## Download Classifier and Test Audio\n",
        "\n",
        "Download the ELL classifier, featurizer, and associated header files below. Unzip the download and place the files in your Azure Sphere project.\n",
        "\n",
        "If you wish, you can also specify and download an audio sample to test on the Azure Sphere. A header file containing the raw audio data will be generated. Note that the audio sample will be truncated to about one second to reduce the file size so it can be run on the Sphere."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiEHOqQ152dT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Download Compiled Featurizer and Classifier\n",
        "import tempfile\n",
        "import time\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "zip_name = \"audio_classifier.zip\" #@param {type: \"string\"}\n",
        "\n",
        "with tempfile.TemporaryDirectory() as tempdir:\n",
        "  with zipfile.ZipFile(zip_name, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
        "      zipf.write(train_dir / \"classifier.h\", \"classifier.h\")\n",
        "      zipf.write(train_dir / \"classifier.o\", \"classifier.o\")\n",
        "      zipf.write(train_dir / \"featurizer.h\", \"featurizer.h\")\n",
        "      zipf.write(train_dir / \"featurizer.o\", \"featurizer.o\")\n",
        "  time.sleep(1)\n",
        "  files.download(zip_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9TSBHaxz9dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Download Test Audio Sample {display-mode: \"form\"}\n",
        "import tempfile\n",
        "import time\n",
        "from scipy.io import wavfile\n",
        "from google.colab import files\n",
        "\n",
        "category = \"window_break\" #@param ['background_noise', 'door_break', 'gunshot', 'window_break']\n",
        "audio_filename = \"452667_window_break.wav\" #@param {type: \"string\"}\n",
        "audio_file = train_data_dir / category / audio_filename\n",
        "frame_size = 512\n",
        "max_rows = 35\n",
        "output_filename = category + \".h\"\n",
        "output_audio_filename = category + \".wav\"\n",
        "\n",
        "fs, data = wavfile.read(audio_file)\n",
        "with tempfile.TemporaryDirectory() as tempdir:\n",
        "  output_file = Path(tempdir) / output_filename\n",
        "  output_audio = Path(tempdir) / output_audio_filename\n",
        "  with open(output_file, mode='w') as f:\n",
        "    f.write(\"short sample_wav_data[][AUDIO_FRAME_SIZE] = {\\n\")\n",
        "    f.write(\"{\")\n",
        "    for i, point in enumerate(data):\n",
        "      if i >= max_rows * frame_size:\n",
        "        break\n",
        "      if i % frame_size == 0 and i != 0:\n",
        "        f.write(f\"}},\\n{{{point:.1f}, \")\n",
        "      else:\n",
        "        f.write(f\"{point:.1f}, \")\n",
        "    extraPaddingLength = frame_size - ((i - 1) % frame_size) - 1\n",
        "    for _ in range(extraPaddingLength):\n",
        "      f.write(\"0.0, \")\n",
        "    f.write(\"},\\n};\")\n",
        "    wavfile.write(output_audio, fs, data[:i])\n",
        "\n",
        "  time.sleep(0.5)\n",
        "  files.download(output_file)\n",
        "  files.download(output_audio)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}